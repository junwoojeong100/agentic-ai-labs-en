{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eea5182",
   "metadata": {},
   "source": [
    "# Lab 7: Agent Evaluation\n",
    "\n",
    "This notebook evaluates the quality and performance of deployed Agents.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Agent evaluation using Azure AI Evaluation SDK\n",
    "- Collect performance, accuracy, and safety metrics\n",
    "- Analyze and visualize evaluation results\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Completed Notebooks 01-03 (Azure resources and Agent deployment)\n",
    "2. `config.json` file exists\n",
    "3. Azure AI Project access permissions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376b3082",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚öôÔ∏è Before You Start\n",
    "\n",
    "**Select a Python kernel:**\n",
    "\n",
    "1. Click **\"Select Kernel\"** in the top right of the notebook\n",
    "2. Select **\"Python Environments...\"**\n",
    "3. Select **`.venv (Python 3.x.x)`** (virtual environment created in project root)\n",
    "\n",
    "> üí° **GitHub Codespaces**: In Codespaces, the `.venv` environment is automatically created.  \n",
    "> If you don't see `.venv`, create it in the terminal with `python -m venv .venv`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784224b9",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Load Config\n",
    "\n",
    "Load the configuration generated in previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eebcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Load config.json\n",
    "config_path = Path(\"config.json\")\n",
    "if not config_path.exists():\n",
    "    raise FileNotFoundError(\"config.json not found. Please run notebooks 01-03 first.\")\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Extract key variables\n",
    "PROJECT_CONNECTION_STRING = config.get(\"project_connection_string\", \"\")\n",
    "simple_project_conn = PROJECT_CONNECTION_STRING.split(';')[0] if PROJECT_CONNECTION_STRING else \"\"\n",
    "\n",
    "print(\"=== Configuration Loaded ===\")\n",
    "print(f\"Resource Group: {config.get('resource_group')}\")\n",
    "print(f\"Location: {config.get('location')}\")\n",
    "print(f\"Project: {simple_project_conn}\")\n",
    "print(f\"Model: {config.get('model_deployment_name')}\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc8337",
   "metadata": {},
   "source": [
    "## 2. Install Azure AI Evaluation Package\n",
    "\n",
    "Install packages required for Agent evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0861df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Azure AI Evaluation package\n",
    "print(\"=== Installing Azure AI Evaluation Package ===\\n\")\n",
    "\n",
    "result = subprocess.run(\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"azure-ai-evaluation\"],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"‚úÖ azure-ai-evaluation installed successfully\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Installation warning: {result.stderr}\")\n",
    "    print(\"   Proceeding anyway...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478b155d",
   "metadata": {},
   "source": [
    "## 3. Evaluation Overview\n",
    "\n",
    "### Evaluation Types\n",
    "\n",
    "**Performance Evaluators:**\n",
    "- **Intent Resolution**: Evaluate if the Agent correctly understood user intent\n",
    "- **Tool Call Accuracy**: Evaluate if the Agent correctly called the right tools  \n",
    "- **Task Adherence**: Evaluate if the Agent faithfully followed instructions\n",
    "\n",
    "**Safety Evaluators:**\n",
    "- **Content Safety**: Evaluate for inappropriate content (violence, hate, etc.)\n",
    "- **Indirect Attack**: Detect indirect malicious attack attempts\n",
    "- **Code Vulnerability**: Evaluate security vulnerabilities in generated code\n",
    "\n",
    "### Evaluation Process\n",
    "\n",
    "1. Generate test queries\n",
    "2. Create Agent and execute queries\n",
    "3. Collect responses and measure metrics\n",
    "4. Evaluate quality with Evaluators\n",
    "5. Save and analyze results\n",
    "6. Clean up Agent\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a4deab",
   "metadata": {},
   "source": [
    "## 4. Generate Test Queries\n",
    "\n",
    "Generate queries to test various Agent capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaa486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evals directory\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "evals_dir = Path(\"evals\")\n",
    "evals_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=== Creating Evaluation Test Queries ===\\n\")\n",
    "\n",
    "# Define test queries\n",
    "eval_queries = [\n",
    "    {\n",
    "        \"query\": \"Hello. I would like to get travel destination recommendations.\",\n",
    "        \"ground-truth\": \"Agent should respond to greeting and call Research Agent for travel recommendations.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Please tell me the current weather in Busan. Include temperature and feels-like temperature.\",\n",
    "        \"ground-truth\": \"Must provide accurate current weather information for Busan, including both temperature and feels-like temperature.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are some travel destinations in Jeju Island that I can enjoy with my family?\",\n",
    "        \"ground-truth\": \"Should search and recommend family-friendly destinations in Jeju Island. Provide information considering natural attractions, experiential activities, accessibility, etc.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Please recommend beaches where I can surf.\",\n",
    "        \"ground-truth\": \"Should search and recommend beaches in Korea where surfing is possible. Provide information about surfing spots like Yangyang, Busan, etc.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are good autumn foliage spots to visit in fall?\",\n",
    "        \"ground-truth\": \"Should search and recommend good autumn foliage spots to visit in fall. Provide information about natural attractions like Naejangsan, Seoraksan, etc.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save as JSON file\n",
    "eval_queries_path = evals_dir / \"eval-queries.json\"\n",
    "with open(eval_queries_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(eval_queries, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ Created {eval_queries_path}\")\n",
    "print(f\"\\nüìã Test Queries ({len(eval_queries)} total):\\n\")\n",
    "\n",
    "for i, query in enumerate(eval_queries, 1):\n",
    "    print(f\"   {i}. {query['query'][:60]}...\")\n",
    "\n",
    "print(\"\\nüí° Each query tests different Agent capabilities:\")\n",
    "print(\"   ‚Ä¢ General conversation and travel intent understanding\")\n",
    "print(\"   ‚Ä¢ Weather lookup (Tool functionality)\")\n",
    "print(\"   ‚Ä¢ Travel destination knowledge search (RAG functionality)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ded78b6",
   "metadata": {},
   "source": [
    "## 5. Run Agent Evaluation\n",
    "\n",
    "Create evaluation Agent, execute test queries, and evaluate quality with Evaluators.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Execution time**: Takes approximately 2-3 minutes\n",
    "- **Region constraints**: Some Safety Evaluators are not supported in eastus region\n",
    "- **Permission issues**: Azure AI Project storage permissions may be required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d18ef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Agent Evaluation\n",
    "import time\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "from azure.ai.agents.models import RunStatus, MessageRole\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.evaluation import (\n",
    "    AIAgentConverter, evaluate, ToolCallAccuracyEvaluator, IntentResolutionEvaluator, \n",
    "    TaskAdherenceEvaluator\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "print(\"=== Running Agent Evaluation ===\\n\")\n",
    "\n",
    "# Set file paths\n",
    "current_dir = Path(\".\")\n",
    "evals_dir = current_dir / \"evals\"\n",
    "eval_queries_path = evals_dir / \"eval-queries.json\"\n",
    "eval_input_path = evals_dir / \"eval-input.jsonl\"\n",
    "eval_output_path = evals_dir / \"eval-output.json\"\n",
    "\n",
    "# Load environment variables (already loaded from config)\n",
    "project_endpoint = simple_project_conn\n",
    "parsed_endpoint = urlparse(project_endpoint)\n",
    "model_endpoint = f\"{parsed_endpoint.scheme}://{parsed_endpoint.netloc}\"\n",
    "deployment_name = config.get(\"model_deployment_name\", \"gpt-4o\")\n",
    "\n",
    "print(f\"üìã Configuration:\")\n",
    "print(f\"   Project: {project_endpoint}\")\n",
    "print(f\"   Model: {deployment_name}\")\n",
    "print(f\"   Test Queries: {eval_queries_path}\")\n",
    "print(f\"\\n\")\n",
    "\n",
    "# Initialize AIProjectClient\n",
    "print(\"üîå Connecting to AI Project...\")\n",
    "credential = DefaultAzureCredential()\n",
    "ai_project = AIProjectClient(\n",
    "    credential=credential,\n",
    "    endpoint=project_endpoint,\n",
    "    api_version=\"2025-05-15-preview\"  # Evaluations require preview API\n",
    ")\n",
    "print(\"‚úÖ Connected\\n\")\n",
    "\n",
    "# Create evaluation Agent\n",
    "print(\"ü§ñ Creating Evaluation Agent...\")\n",
    "eval_agent = ai_project.agents.create_agent(\n",
    "    model=deployment_name,\n",
    "    name=\"Evaluation Agent\",\n",
    "    instructions=\"\"\"You are a helpful travel and weather assistant.\n",
    "    \n",
    "You can help users with:\n",
    "1. Travel recommendations and destination information\n",
    "2. Weather information for any city\n",
    "3. General travel planning advice\n",
    "\n",
    "Be friendly, informative, and provide detailed responses.\"\"\"\n",
    ")\n",
    "print(f\"‚úÖ Agent created: {eval_agent.name} (ID: {eval_agent.id})\\n\")\n",
    "\n",
    "# Setup evaluation config\n",
    "api_version = config.get(\"api_version\", \"2024-08-01-preview\")\n",
    "model_config = {\n",
    "    \"azure_deployment\": deployment_name,\n",
    "    \"azure_endpoint\": model_endpoint,\n",
    "    \"api_version\": api_version,\n",
    "}\n",
    "\n",
    "thread_data_converter = AIAgentConverter(ai_project)\n",
    "\n",
    "# Execute test queries and prepare evaluation input\n",
    "print(\"=\"*70)\n",
    "print(\"üìù Executing Test Queries\\n\")\n",
    "\n",
    "with open(eval_queries_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"   Total queries: {len(test_data)}\\n\")\n",
    "\n",
    "with open(eval_input_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for idx, row in enumerate(test_data, 1):\n",
    "        query_text = row.get(\"query\")\n",
    "        print(f\"   [{idx}/{len(test_data)}] {query_text[:50]}...\")\n",
    "        \n",
    "        # Create new thread (isolate each query)\n",
    "        thread = ai_project.agents.threads.create()\n",
    "        \n",
    "        # Create user query\n",
    "        ai_project.agents.messages.create(\n",
    "            thread.id, role=MessageRole.USER, content=query_text\n",
    "        )\n",
    "        \n",
    "        # Run Agent and measure performance\n",
    "        start_time = time.time()\n",
    "        run = ai_project.agents.runs.create_and_process(\n",
    "            thread_id=thread.id, agent_id=eval_agent.id\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        \n",
    "        if run.status != RunStatus.COMPLETED:\n",
    "            print(f\"      ‚ö†Ô∏è  Run failed: {run.last_error or 'Unknown error'}\")\n",
    "            continue\n",
    "        \n",
    "        # Collect operational metrics\n",
    "        operational_metrics = {\n",
    "            \"server-run-duration-in-seconds\": (\n",
    "                run.completed_at - run.created_at\n",
    "            ).total_seconds(),\n",
    "            \"client-run-duration-in-seconds\": end_time - start_time,\n",
    "            \"completion-tokens\": run.usage.completion_tokens,\n",
    "            \"prompt-tokens\": run.usage.prompt_tokens,\n",
    "            \"ground-truth\": row.get(\"ground-truth\", '')\n",
    "        }\n",
    "        \n",
    "        # Add thread data + operational metrics to evaluation input\n",
    "        evaluation_data = thread_data_converter.prepare_evaluation_data(thread_ids=thread.id)\n",
    "        eval_item = evaluation_data[0]\n",
    "        eval_item[\"metrics\"] = operational_metrics\n",
    "        f.write(json.dumps(eval_item, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "        print(f\"      ‚úÖ Completed in {operational_metrics['client-run-duration-in-seconds']:.1f}s\")\n",
    "        print(f\"         Tokens: {operational_metrics['prompt-tokens']} prompt + {operational_metrics['completion-tokens']} completion\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚úÖ All queries executed successfully\")\n",
    "print(f\"   Input saved to: {eval_input_path}\\n\")\n",
    "\n",
    "# Run Evaluation\n",
    "print(\"=\"*70)\n",
    "print(\"üî¨ Running Evaluators\\n\")\n",
    "\n",
    "print(\"   Evaluators:\")\n",
    "print(\"      ‚Ä¢ ToolCallAccuracyEvaluator\")\n",
    "print(\"      ‚Ä¢ IntentResolutionEvaluator\")\n",
    "print(\"      ‚Ä¢ TaskAdherenceEvaluator\")\n",
    "print(\"\\n   ‚ö†Ô∏è  Note: Some RAI evaluators are not supported in eastus region.\")\n",
    "print(\"      (CodeVulnerability, ContentSafety, IndirectAttack are excluded)\\n\")\n",
    "print(\"   ‚è≥ This may take 1-2 minutes...\\n\")\n",
    "\n",
    "# Define OperationalMetricsEvaluator\n",
    "class OperationalMetricsEvaluator:\n",
    "    \"\"\"Propagate operational metrics to the final evaluation results\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, *, metrics: dict, **kwargs):\n",
    "        return metrics\n",
    "\n",
    "# Run Evaluation (locally)\n",
    "print(\"   üí° Saving evaluation results locally.\\n\")\n",
    "\n",
    "results = evaluate(\n",
    "    evaluation_name=\"foundry-agent-evaluation\",\n",
    "    data=eval_input_path,\n",
    "    evaluators={\n",
    "        \"operational_metrics\": OperationalMetricsEvaluator(),\n",
    "        \"tool_call_accuracy\": ToolCallAccuracyEvaluator(model_config=model_config),\n",
    "        \"intent_resolution\": IntentResolutionEvaluator(model_config=model_config),\n",
    "        \"task_adherence\": TaskAdherenceEvaluator(model_config=model_config),\n",
    "        # Evaluators not supported in eastus region are excluded\n",
    "        # \"code_vulnerability\": CodeVulnerabilityEvaluator(credential=credential, azure_ai_project=project_endpoint),\n",
    "        # \"content_safety\": ContentSafetyEvaluator(credential=credential, azure_ai_project=project_endpoint),\n",
    "        # \"indirect_attack\": IndirectAttackEvaluator(credential=credential, azure_ai_project=project_endpoint)\n",
    "    },\n",
    "    output_path=eval_output_path,\n",
    "    # Remove azure_ai_project parameter (ML workspace not needed)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Evaluation completed!\\n\")\n",
    "print(f\"üìÅ Results saved to: {eval_output_path}\")\n",
    "print(f\"   Check results in the next cell.\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Delete Evaluation Agent\n",
    "print(\"üßπ Cleaning up...\")\n",
    "ai_project.agents.delete_agent(eval_agent.id)\n",
    "print(f\"‚úÖ Evaluation Agent deleted: {eval_agent.id}\\n\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208dbb91",
   "metadata": {},
   "source": [
    "## 6. Visualize Evaluation Results\n",
    "\n",
    "Visualize and analyze results with tables and charts.\n",
    "\n",
    "### How to Check Results\n",
    "\n",
    "**Method 1: Check in Notebook**\n",
    "- Run the cell below to view results directly in the notebook.\n",
    "\n",
    "**Method 2: Check in Terminal**\n",
    "- Run the following command in terminal to view the same results:\n",
    "  ```bash\n",
    "  python3 show_eval_results.py\n",
    "  ```\n",
    "- This script is automatically created in the project root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f25ecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed visualization of Evaluation results\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "SEPARATOR = \"‚îÄ\" * 100\n",
    "LINE = \"=\" * 100\n",
    "\n",
    "def get_score_color(score, threshold=3.0):\n",
    "    if score >= 4.5:\n",
    "        return \"\\033[92m\"\n",
    "    elif score >= threshold:\n",
    "        return \"\\033[93m\"\n",
    "    else:\n",
    "        return \"\\033[91m\"\n",
    "\n",
    "def reset_color():\n",
    "    return \"\\033[0m\"\n",
    "\n",
    "def get_score_indicator(score, threshold=3.0):\n",
    "    if score >= 4.5:\n",
    "        return \"‚úÖ\"\n",
    "    elif score >= threshold:\n",
    "        return \"‚ö†Ô∏è\"\n",
    "    else:\n",
    "        return \"‚ùå\"\n",
    "\n",
    "def extract_query_text(query_input):\n",
    "    if isinstance(query_input, list):\n",
    "        for item in query_input:\n",
    "            if isinstance(item, dict) and item.get(\"role\") == \"user\":\n",
    "                content = item.get(\"content\", [])\n",
    "                if isinstance(content, list) and len(content) > 0:\n",
    "                    return content[0].get(\"text\", \"\")\n",
    "    return \"\"\n",
    "\n",
    "def extract_response_text(response):\n",
    "    if isinstance(response, list):\n",
    "        for item in response:\n",
    "            if isinstance(item, dict) and item.get(\"role\") == \"assistant\":\n",
    "                content = item.get(\"content\", [])\n",
    "                if isinstance(content, list) and len(content) > 0:\n",
    "                    return content[0].get(\"text\", \"\")\n",
    "    return \"\"\n",
    "\n",
    "eval_output_path = Path(\"evals/eval-output.json\")\n",
    "\n",
    "print(LINE)\n",
    "print(\"üìä AGENT EVALUATION RESULTS - Detailed Analysis Report\")\n",
    "print(LINE, \"\\n\")\n",
    "\n",
    "if not eval_output_path.exists():\n",
    "    print(\"‚ùå Evaluation results file not found.\")\n",
    "    print(f\"   File path: {eval_output_path.absolute()}\")\n",
    "    print(\"\\n   Please run cell 5 of 07_evaluate_agents.ipynb first.\\n\")\n",
    "else:\n",
    "    with open(eval_output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    metrics = data.get(\"metrics\", {})\n",
    "    rows = data.get(\"rows\", [])\n",
    "    \n",
    "    # Section 1: Overall Average Scores\n",
    "    print(\"‚≠ê Overall Average Performance Scores\")\n",
    "    print(LINE)\n",
    "    scores_config = [\n",
    "        (\"Intent Resolution\", \"intent_resolution.intent_resolution\", \"Intent Understanding\", 3.0),\n",
    "        (\"Task Adherence\", \"task_adherence.task_adherence\", \"Task Fidelity\", 3.0),\n",
    "    ]\n",
    "    \n",
    "    for name, key, desc, threshold in scores_config:\n",
    "        if key in metrics:\n",
    "            score = metrics[key]\n",
    "            color = get_score_color(score, threshold)\n",
    "            reset = reset_color()\n",
    "            indicator = get_score_indicator(score, threshold)\n",
    "            stars = \"‚òÖ\" * int(score) + \"‚òÜ\" * (5 - int(score))\n",
    "            bar = \"‚ñà\" * int(score * 4) + \"‚ñë\" * (20 - int(score * 4))\n",
    "            \n",
    "            print(f\"{indicator} {name:20} {color}{score:.2f}/5.0{reset}  {stars}\")\n",
    "            print(f\"     {desc:20} [{bar}]\")\n",
    "            if score < threshold:\n",
    "                print(f\"     {color}‚ö†Ô∏è Below threshold (baseline: {threshold:.1f}){reset}\")\n",
    "            print()\n",
    "    \n",
    "    # Section 2: Operational Metrics\n",
    "    print(\"\\n‚ö° Operational Metrics (Average)\")\n",
    "    print(LINE)\n",
    "    \n",
    "    operational_keys = [\n",
    "        (\"operational_metrics.server-run-duration-in-seconds\", \"Server Run Duration\", \"s\"),\n",
    "        (\"operational_metrics.client-run-duration-in-seconds\", \"Client Run Duration\", \"s\"),\n",
    "        (\"operational_metrics.prompt-tokens\", \"Prompt Tokens\", \"tokens\"),\n",
    "        (\"operational_metrics.completion-tokens\", \"Completion Tokens\", \"tokens\"),\n",
    "    ]\n",
    "    \n",
    "    total_tokens = 0\n",
    "    for key, desc, unit in operational_keys:\n",
    "        if key in metrics:\n",
    "            value = metrics[key]\n",
    "            if unit == \"tokens\":\n",
    "                print(f\"  {desc:30} {int(value):>10,} {unit}\")\n",
    "                total_tokens += value\n",
    "            else:\n",
    "                print(f\"  {desc:30} {value:>10.2f} {unit}\")\n",
    "    \n",
    "    if total_tokens > 0:\n",
    "        print(f\"  {'Total Token Usage':30} {int(total_tokens):>10,} tokens\")\n",
    "        cost = (total_tokens / 1000) * 0.0025\n",
    "        print(f\"  {'Estimated Cost (GPT-4o)':30} ${cost:>9.4f}\")\n",
    "    print()\n",
    "    \n",
    "    # Section 3: Individual Query Detailed Results\n",
    "    if rows:\n",
    "        print(\"\\nüìã Detailed Results by Query\")\n",
    "        print(LINE)\n",
    "        \n",
    "        for idx, row in enumerate(rows, 1):\n",
    "            query = extract_query_text(row.get(\"inputs.query\", []))\n",
    "            response = extract_response_text(row.get(\"inputs.response\", []))\n",
    "            ground_truth = row.get(\"inputs.metrics.ground-truth\", \"\")\n",
    "            \n",
    "            print(f\"\\n{SEPARATOR}\")\n",
    "            print(f\"üîç Query #{idx}\")\n",
    "            print(SEPARATOR)\n",
    "            \n",
    "            print(\"\\nüí¨ User Question:\")\n",
    "            print(f\"   {query}\")\n",
    "            \n",
    "            if ground_truth:\n",
    "                print(\"\\nüìå Expected Behavior (Ground Truth):\")\n",
    "                print(f\"   {ground_truth}\")\n",
    "            \n",
    "            if response:\n",
    "                print(\"\\nü§ñ Agent Response (Summary):\")\n",
    "                response_preview = response[:200] if len(response) > 200 else response\n",
    "                lines_shown = 0\n",
    "                for line in response_preview.split(\"\\n\"):\n",
    "                    if line.strip() and lines_shown < 3:\n",
    "                        print(f\"   {line.strip()}\")\n",
    "                        lines_shown += 1\n",
    "                if len(response) > 200:\n",
    "                    print(f\"   ... (total {len(response):,} characters)\")\n",
    "            \n",
    "            print(\"\\nüìä Evaluation Scores:\")\n",
    "            \n",
    "            intent = row.get(\"outputs.intent_resolution.intent_resolution\", \"N/A\")\n",
    "            task = row.get(\"outputs.task_adherence.task_adherence\", \"N/A\")\n",
    "            tool = row.get(\"outputs.tool_call_accuracy.tool_call_accuracy\", \"N/A\")\n",
    "            \n",
    "            intent_threshold = row.get(\"outputs.intent_resolution.intent_resolution_threshold\", 3)\n",
    "            task_threshold = row.get(\"outputs.task_adherence.task_adherence_threshold\", 3)\n",
    "            \n",
    "            if isinstance(intent, (int, float)):\n",
    "                color = get_score_color(intent, intent_threshold)\n",
    "                reset = reset_color()\n",
    "                indicator = get_score_indicator(intent, intent_threshold)\n",
    "                print(f\"   {indicator} Intent Resolution:  {color}{intent:.1f}/5.0{reset} (threshold: {intent_threshold})\")\n",
    "            else:\n",
    "                print(f\"   ‚Ä¢ Intent Resolution:  {intent}\")\n",
    "            \n",
    "            if isinstance(task, (int, float)):\n",
    "                color = get_score_color(task, task_threshold)\n",
    "                reset = reset_color()\n",
    "                indicator = get_score_indicator(task, task_threshold)\n",
    "                print(f\"   {indicator} Task Adherence:     {color}{task:.1f}/5.0{reset} (threshold: {task_threshold})\")\n",
    "            else:\n",
    "                print(f\"   ‚Ä¢ Task Adherence:     {task}\")\n",
    "            \n",
    "            print(f\"   ‚Ä¢ Tool Call Accuracy: {tool}\")\n",
    "            \n",
    "            # Evaluation reasoning\n",
    "            print(\"\\nüí° Evaluation Details:\")\n",
    "            \n",
    "            intent_reason = row.get(\"outputs.intent_resolution.intent_resolution_reason\", \"\")\n",
    "            task_reason = row.get(\"outputs.task_adherence.task_adherence_reason\", \"\")\n",
    "            tool_reason = row.get(\"outputs.tool_call_accuracy.tool_call_accuracy_reason\", \"\")\n",
    "            \n",
    "            if intent_reason:\n",
    "                print(\"\\n   [Intent Resolution Reasoning]\")\n",
    "                for sentence in intent_reason.split(\". \"):\n",
    "                    if sentence.strip():\n",
    "                        print(f\"   ‚Ä¢ {sentence.strip()}.\")\n",
    "            \n",
    "            if task_reason:\n",
    "                print(\"\\n   [Task Adherence Reasoning]\")\n",
    "                for sentence in task_reason.split(\". \"):\n",
    "                    if sentence.strip():\n",
    "                        print(f\"   ‚Ä¢ {sentence.strip()}.\")\n",
    "            \n",
    "            if tool_reason:\n",
    "                print(\"\\n   [Tool Call Accuracy Reasoning]\")\n",
    "                for sentence in tool_reason.split(\". \"):\n",
    "                    if sentence.strip():\n",
    "                        print(f\"   ‚Ä¢ {sentence.strip()}.\")\n",
    "            \n",
    "            duration = row.get(\"outputs.operational_metrics.client-run-duration-in-seconds\", 0)\n",
    "            prompt_tokens = row.get(\"outputs.operational_metrics.prompt-tokens\", 0)\n",
    "            completion_tokens = row.get(\"outputs.operational_metrics.completion-tokens\", 0)\n",
    "            \n",
    "            print(\"\\n‚è±Ô∏è  Performance Metrics:\")\n",
    "            print(f\"   ‚Ä¢ Execution Time: {duration:.2f}s\")\n",
    "            print(f\"   ‚Ä¢ Token Usage: {prompt_tokens:,} (input) + {completion_tokens:,} (output) = {prompt_tokens + completion_tokens:,} (total)\")\n",
    "            \n",
    "            issues = []\n",
    "            if isinstance(intent, (int, float)) and intent < intent_threshold:\n",
    "                issues.append(f\"Low Intent Resolution score ({intent:.1f} < {intent_threshold})\")\n",
    "            if isinstance(task, (int, float)) and task < task_threshold:\n",
    "                issues.append(f\"Low Task Adherence score ({task:.1f} < {task_threshold})\")\n",
    "            \n",
    "            if issues:\n",
    "                print(f\"\\n{get_score_color(1.0, 3.0)}‚ö†Ô∏è  Issues Found:{reset_color()}\")\n",
    "                for issue in issues:\n",
    "                    print(f\"   ‚Ä¢ {issue}\")\n",
    "        \n",
    "        # Section 4: Statistical Summary\n",
    "        print(f\"\\n{SEPARATOR}\\n\")\n",
    "        print(\"\\nüìà Statistical Summary and Analysis\")\n",
    "        print(LINE)\n",
    "        \n",
    "        intent_scores = []\n",
    "        task_scores = []\n",
    "        durations = []\n",
    "        total_tokens_list = []\n",
    "        failed_queries = []\n",
    "        \n",
    "        for idx, row in enumerate(rows, 1):\n",
    "            intent = row.get(\"outputs.intent_resolution.intent_resolution\")\n",
    "            task = row.get(\"outputs.task_adherence.task_adherence\")\n",
    "            duration = row.get(\"outputs.operational_metrics.client-run-duration-in-seconds\", 0)\n",
    "            prompt = row.get(\"outputs.operational_metrics.prompt-tokens\", 0)\n",
    "            completion = row.get(\"outputs.operational_metrics.completion-tokens\", 0)\n",
    "            \n",
    "            intent_threshold = row.get(\"outputs.intent_resolution.intent_resolution_threshold\", 3)\n",
    "            task_threshold = row.get(\"outputs.task_adherence.task_adherence_threshold\", 3)\n",
    "            \n",
    "            if isinstance(intent, (int, float)):\n",
    "                intent_scores.append(intent)\n",
    "                if intent < intent_threshold:\n",
    "                    query = extract_query_text(row.get(\"inputs.query\", []))\n",
    "                    failed_queries.append((idx, \"Intent Resolution\", intent, query[:50]))\n",
    "            \n",
    "            if isinstance(task, (int, float)):\n",
    "                task_scores.append(task)\n",
    "                if task < task_threshold:\n",
    "                    query = extract_query_text(row.get(\"inputs.query\", []))\n",
    "                    failed_queries.append((idx, \"Task Adherence\", task, query[:50]))\n",
    "            \n",
    "            if duration:\n",
    "                durations.append(duration)\n",
    "            total_tokens_list.append(prompt + completion)\n",
    "        \n",
    "        if intent_scores:\n",
    "            avg_intent = sum(intent_scores) / len(intent_scores)\n",
    "            color = get_score_color(avg_intent, 3.0)\n",
    "            reset = reset_color()\n",
    "            pass_count = len([s for s in intent_scores if s >= 3.0])\n",
    "            \n",
    "            print(\"\\nüìä Intent Resolution\")\n",
    "            print(f\"   Average: {color}{avg_intent:.2f}/5.0{reset}\")\n",
    "            print(f\"   Max: {max(intent_scores):.1f}  |  Min: {min(intent_scores):.1f}\")\n",
    "            print(f\"   Pass Rate: {pass_count}/{len(intent_scores)} ({pass_count/len(intent_scores)*100:.1f}%)\")\n",
    "        \n",
    "        if task_scores:\n",
    "            avg_task = sum(task_scores) / len(task_scores)\n",
    "            color = get_score_color(avg_task, 3.0)\n",
    "            reset = reset_color()\n",
    "            pass_count = len([s for s in task_scores if s >= 3.0])\n",
    "            \n",
    "            print(\"\\nüìä Task Adherence\")\n",
    "            print(f\"   Average: {color}{avg_task:.2f}/5.0{reset}\")\n",
    "            print(f\"   Max: {max(task_scores):.1f}  |  Min: {min(task_scores):.1f}\")\n",
    "            print(f\"   Pass Rate: {pass_count}/{len(task_scores)} ({pass_count/len(task_scores)*100:.1f}%)\")\n",
    "        \n",
    "        if durations:\n",
    "            print(\"\\n‚è±Ô∏è  Execution Time\")\n",
    "            print(f\"   Average: {sum(durations)/len(durations):.2f}s\")\n",
    "            print(f\"   Max: {max(durations):.2f}s  |  Min: {min(durations):.2f}s\")\n",
    "        \n",
    "        if total_tokens_list:\n",
    "            avg_tokens = sum(total_tokens_list) / len(total_tokens_list)\n",
    "            total_all_tokens = sum(total_tokens_list)\n",
    "            \n",
    "            print(\"\\nüí∞ Token Usage\")\n",
    "            print(f\"   Average: {avg_tokens:,.0f} tokens/query\")\n",
    "            print(f\"   Total: {total_all_tokens:,} tokens\")\n",
    "            print(f\"   Max: {max(total_tokens_list):,}  |  Min: {min(total_tokens_list):,}\")\n",
    "            print(f\"   Estimated Cost (GPT-4o): ${(total_all_tokens / 1000) * 0.0025:.4f}\")\n",
    "        \n",
    "        if failed_queries:\n",
    "            print(f\"\\n{get_score_color(1.0, 3.0)}‚ö†Ô∏è  Queries Needing Improvement ({len(failed_queries)} items){reset_color()}\")\n",
    "            print(SEPARATOR)\n",
    "            \n",
    "            seen = set()\n",
    "            for idx, metric, score, query in failed_queries:\n",
    "                key = (idx, metric)\n",
    "                if key not in seen:\n",
    "                    seen.add(key)\n",
    "                    print(f\"   Query #{idx}: {metric} = {score:.1f}\")\n",
    "                    print(f\"   ‚îî‚îÄ {query}...\")\n",
    "                    print()\n",
    "        else:\n",
    "            print(\"\\n‚úÖ All queries passed the threshold!\")\n",
    "    \n",
    "    print(f\"\\n{LINE}\")\n",
    "    print(f\"‚úÖ Evaluated {len(rows)} queries in total\")\n",
    "    print(f\"üìÅ Detailed JSON: {eval_output_path.absolute()}\")\n",
    "    print(f\"üí° Can also run in terminal: python3 show_eval_results.py\")\n",
    "    print(f\"{LINE}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4900276",
   "metadata": {},
   "source": [
    "## 7. Evaluation Metrics Interpretation Guide\n",
    "\n",
    "### Metric Interpretation\n",
    "\n",
    "**Operational Metrics:**\n",
    "- `server-run-duration-in-seconds`: Agent execution time on server\n",
    "- `client-run-duration-in-seconds`: Total elapsed time measured by client\n",
    "- `prompt-tokens`: Number of input tokens\n",
    "- `completion-tokens`: Number of generated tokens\n",
    "\n",
    "**Performance Metrics:**\n",
    "- `tool_call_accuracy.*`: Tool call accuracy (1-5 points)\n",
    "- `intent_resolution.*`: Intent understanding accuracy (1-5 points)\n",
    "- `task_adherence.*`: Task adherence level (1-5 points)\n",
    "\n",
    "**Score Interpretation:**\n",
    "- 5 points: Perfect\n",
    "- 4 points: Good\n",
    "- 3 points: Average\n",
    "- 2 points: Needs improvement\n",
    "- 1 point: Very poor\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e1fd41",
   "metadata": {},
   "source": [
    "## 8. Next Steps and Agent Improvement\n",
    "\n",
    "### Agent Improvement Methods\n",
    "\n",
    "1. **Analyze Low Scores**\n",
    "   - Identify queries with low scores\n",
    "   - Determine which evaluators had issues\n",
    "\n",
    "2. **Improve Prompts**\n",
    "   - Write clearer Agent instructions\n",
    "   - Add examples\n",
    "   - Specify constraints\n",
    "\n",
    "3. **Improve Functionality**\n",
    "   - Add or improve Tools\n",
    "   - Enhance RAG knowledge base\n",
    "   - Adjust Multi-Agent configuration\n",
    "\n",
    "4. **Re-evaluate**\n",
    "   - Re-evaluate with same queries\n",
    "   - Compare score changes\n",
    "   - Continuous improvement\n",
    "\n",
    "### References\n",
    "\n",
    "- [Azure AI Foundry Agent Evaluation](https://learn.microsoft.com/azure/ai-foundry/how-to/develop/agent-evaluate-sdk)\n",
    "- [Built-in Evaluators](https://learn.microsoft.com/azure/ai-foundry/how-to/develop/evaluate-sdk)\n",
    "- [Evaluation Best Practices](https://learn.microsoft.com/azure/ai-foundry/concepts/evaluation-approach-gen-ai)\n",
    "\n",
    "---\n",
    "\n",
    "## Complete!\n",
    "\n",
    "Congratulations! You have successfully completed Agent Evaluation. üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
